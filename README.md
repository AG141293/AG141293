ğŸ‘‹ Hi, Iâ€™m Ankita Ghosh

ğŸš€ Data Science & AI Engineer | LLM, VLM & NLP Practitioner | GenAI Pipeline Builder

Iâ€™m passionate about transforming raw, noisy data into high-quality AI-ready datasets and building scalable, production-grade AI systems using Machine Learning, NLP, Large Language Models (LLMs), Visionâ€“Language Models (VLMs), and Generative AI.

I actively work on end-to-end GenAI pipelines â€” from data cleaning and validation to synthetic data generation, evaluation, and automation â€” with a strong focus on real-world deployment and multilingual AI systems.

ğŸ”¹ About Me

ğŸ“ Background: Artificial Intelligence & Machine Learning
ğŸ’» Core Skills: Python, Data Cleaning, Machine Learning, Deep Learning, NLP, GenAI
ğŸ¤– Focus Areas:

Large Language Models (LLMs)

Visionâ€“Language Models (VLMs)

Transformers

Prompt Engineering

Synthetic Data Generation (SDG)

GenAI Automation Pipelines

ğŸ“Š Case Studies: Walmart, Ola, LoanTap, Delhivery, Netflix, and more
ğŸŒ± Motto: Always learning, always curious!

ğŸ’¼ Experience
Machine Learning Intern â€” Makers Lab, Tech Mahindra

Project: INDUS â€” Indiaâ€™s Own Large Language Model (LLM)

Worked on large-scale multilingual data preparation and augmentation for Indian language AI systems.

ğŸ”¹ Data Cleaning & Normalization (Multilingual NLP)

Designed and implemented a robust data-cleaning pipeline for Hindi and mixed-language (Hindi + English) datasets

Cleaned educational and concept-based datasets containing questions, answers, and explanations

Performed:

Text normalization (whitespace, line breaks, encoding issues)

Removal of noisy tokens and unwanted artifacts

Mixed-language handling with selective translation

Mathematical symbol, unit, and number protection to avoid semantic corruption

Conversion of Hindi numerals to English digits

Built logic to preserve formulas, units, operators, and measurements

Ensured CSV encoding safety across UTF-8, UTF-16, and legacy formats

Produced clean, model-ready datasets suitable for training and evaluation of LLMs

ğŸ”¹ Synthetic Data Generation (10Ã— Dataset Expansion)

Built LLM-driven Synthetic Data Generation (SDG) pipelines using NVIDIA NIM (LLaMA-3.1-70B)

Expanded seed datasets by 10Ã— (from ~3K to 30K+ rows) while preserving semantic consistency

Generated high-quality Hindi MCQs with:

Difficulty levels (easy / medium / hard)

Cognitive levels (recall, understanding, application)

Concept-wise alignment

Designed strict JSON-only output validation to ensure downstream usability

Implemented:

Retry logic & failure handling

Checkpoint-based saving for long-running jobs

Resume-from-failure capability

Cleaned and corrected LLM output artifacts (escaping issues, LaTeX formatting, malformed JSON)

ğŸ”¹ Core-Conceptâ€“Driven SDG

Generated synthetic questions anchored to core physics concepts

Ensured concept fidelity by conditioning generation on:

Concept name

Reference question

Educational grade constraints

Maintained pedagogical correctness while increasing dataset diversity

Used SDG outputs for training, evaluation, and benchmarking educational NLP models

ğŸ”¹ LLM, NLP & Speech Systems

Curated and processed multilingual Indian dialect datasets

Designed pipelines for:

Dialect classification

Annotation

Validation

Created dialectal variants datasets (10,000+ rows)

Evaluated model performance and accuracy

Implemented Automatic Speech Recognition (ASR) pipelines

Measured transcription accuracy using Word Error Rate (WER) (jiwer)

Integrated real-time multilingual translation

Gained hands-on experience with Indian language AI systems

ğŸ§  GenAI, LLM & VLM Projects
ğŸ”¹ Q&A Generation using LLM Pipelines

Built a fully automated Questionâ€“Answer generation system using Gemini Flash 2.5.

Pipeline:
Text â†’ Chunking â†’ Question Generation â†’ Answer Extraction â†’ Validation

Designed structured prompt templates

Generated:

MCQs

Short-answer questions

Long descriptive answers

Interview-style questions

Implemented semantic similarity scoring for output validation

Built modular, reusable, production-ready pipelines

ğŸ”¹ Visionâ€“Language Model (VLM) Pipelines

Worked on Vision + Text AI workflows combining OCR, document understanding, and LLMs.

Extracted text from images and scanned PDFs using OCR

Processed visual documents into structured, machine-readable formats

Combined visual context + textual understanding for:

Document comprehension

Information extraction

Downstream Q&A generation

Integrated VLM outputs into end-to-end GenAI pipelines

ğŸ”¹ File Conversion & AI Automation Pipelines

Developed reusable automation utilities for:

PDF â†’ Text / JSON

Image â†’ Text (OCR)

CSV â†” Excel

Text cleaning & normalization

GenAI-based document understanding

ğŸ”¹ Document Reading & Summarization Pipelines

Built intelligent document workflows:

Pipeline:
Document â†’ Chunking â†’ Summary â†’ Explanation â†’ Keywords

Section-wise summarization using LLMs

Semantic search for context-aware retrieval

Automatic extraction of:

Key insights

Highlights

Glossaries

Action points

ğŸŒ Frontend + AI Integration (HTML-Based UI)

Designed HTML-based interfaces for AI workflows:

OCR & document ingestion

Data cleaning & normalization

Q&A generation

Web scraping

File conversion

Enabled end-to-end AI pipelines with user-friendly interaction.

ğŸ› ï¸ Tech Stack
Languages

Python

SQL

Frameworks & Libraries

TensorFlow

PyTorch

Pandas

NumPy

Scikit-learn

EasyOCR

jiwer

deep_translator

indic-nlp-library

GenAI / LLM / VLM

Gemini Flash 2.5

NVIDIA NIM

LLaMA-3.1

Prompt Engineering

LLM & VLM Pipelines

Synthetic Data Generation

Tools

GitHub

VS Code

Google Colab

Jupyter Notebook

ğŸ“ Connect with me
ğŸ”— www.linkedin.com/in/ank1412

